\chapter{Calculus of variations}

\begin{dfn}
  [Variational problem]

  Let $X$ be a real vector space (of functions, possibly infinite dimensional),
  $\calA \subset X$ a set of admissible functions and $\calI : X \to \R$ a
  functional that assigns a real number for each $u \in X$.

  A variational problem is the task
  \[
    \text{minimise } \calI(u),\quad u \in \calA.
  \]
\end{dfn}

In particular we are concerned with variational problems of integral form, i.e.
those where
\begin{align}
  \label{eq:cv-integral}
  \calI(u) = \int_a^b f(x, u(x), u'(x))dx,
\end{align}
where $u : [a, b] \to \R^m$ and $f: [a,b] \times \R^m \times \R^m \to \R$.

As mathematicians, we are immediately concerned about the existence and
uniqueness of solutions. Until 1850, it was thought that minimisers for
integral problems always existed but Weierstrass gave a counter example. From
that moment on, a new theory for solving variational problems, now known as the
\textbf{direct method} was developed. In this section we will mostly
concentrate on the classical or \textbf{indirect method} for finding minimisers
which basically replicates the process of finding minima for functions in
vector calculus. The reason for this is that a formal treatment of the direct
method requires advanced mathematical tools from functional analysis, which is
not a prerequisite for this course.

\paragraph{A word about notation.} As usual during this course, notation is a
mess. Now we are using the notation $u'(x)$ to represent the derivative of a
vector-valued one-variable function $u : \R \to \R^m$. In formal analysis one
would write this as
\[
  Du(x) =
  \begin{pmatrix}
    \partial_x u_1(x) \\
    \partial_x u_2(x) \\
    \vdots \\
    \partial_x u_m(x)
  \end{pmatrix} =
  \begin{pmatrix}
    \ddx u_1(x) \\ \ddx u_2(x) \\ \vdots \\ u_m(x)
  \end{pmatrix}.
\]
I think it may be the heavy influence of geometry in this field that leads us
to use the \textit{prime} notation as is normally in analytic geometry.

Also, as usual, the notion of row and column vectors is a bit blurry and we
will just assume that they have the right shape as needed. Clarification will
be provided when the notation is unclear.


\section{The indirect method}

This method was developed by Euler and Lagrange during the 18th century. The
strategy for dealing with variational problems of the form
\eqref{eq:cv-integral} is to just go ahead and find the minima of the function
$\calI(u)$. For this, let us recall the approach taken in vector calculus to
minimise a function $f: \R^m \to \R^n$.
\begin{enumerate}
  \item Find $\overline{x} \in \R^m$ such that $Df(\overline{x}) = \0$,
  \item Find $D^2f(\overline{x})$, and
  \item Check that $D^2f(\overline{x})$ is positive definite.
\end{enumerate}

The problem with this strategy is that we don't know if $Df$ or $D^2f$ will
exist for our functional $f = \calI$. Therefore, we will introduce a weaker
notion of derivative, the variation, which is easier to work with in the
context of these problems.

\paragraph{Warning.} Scrutinise the results given in this chapter regarding the
necessary and sufficient conditions for the existence of local minimisers. In
particular, a positive second derivative on a critical point does not imply
that critical point is a local minimiser. See example TODO and theorem TODO.

\begin{dfn}
  [Variation, general definition]

  Let $X$ be a function space and $J: X \to \R$ be a functional over $X$. We
  define the $k$-th variation of the functional $J$ at $u \in X$ in the
  direction of $\varphi \in X$ as the limit
  \[
    \delta^k J(u)(\varphi)
    = \left. \frac{d^k}{d\varepsilon^k} J(y + \varepsilon \varphi)\right|_{\varepsilon = 0}.
  \]
\end{dfn}

In our setting, where we only want to consider admissible functions, we use a
version of the definition of variation which incorporates this restriction.

\begin{dfn}
  [Admissible perturbation]

  Let $X$ and $\calA$ be a function space and a set of admissible functions,
  resp. Let $u \in \calA$. For any $\varphi \in X$ we say $\varphi$ is an
  admissible perturvation of $u$ iff there exists a $\varepsilon_0 > 0$ such
  that
  \[
    u + \varepsilon \varphi \in A,\quad \text{ for every } \varepsilon \in
    (-\varepsilon_0, \varepsilon_0).
  \]
\end{dfn}

\begin{dfn}
  [Variation]
  Let $X$ be a function space and let $\calA \subset X$ be the set of
  admissible functions. We define the $k$-th variation of a functional $\calI$
  over $X$ at $u \in \calA$ in the direction of an admissible perturbation
  $\varphi$ as the limit
  \begin{equation}
    \label{eq:variation}
    \delta \calI (u) (\varphi) := 
    \left. \frac{d^k}{d \varepsilon^k} \calI(u + \varepsilon \varphi)\right|_{\varepsilon = 0}.
  \end{equation}
\end{dfn}

Finally, we recall the definition of a local minimiser.

\begin{dfn}
  [Local minimiser]

  Let $X$ be a function space and $\calI : X \to \R$ be a functional over $X$.
  We say $\overline{u} \in X$ is a local minimiser of $\calI$ iff there exists
  some $\delta > 0$ such that
  \[
    \forall u \in X,\ \norm{u - \overline{u}}_X < \delta \implies
      \calI(u) \geq \calI(\overline{u}).
  \]
\end{dfn}

In the previous definition one may use any norm defined over $X$ to compute the
distance between $u$ and $\overline{u}$. For function spaces like $X = \{f : A
\to B\}$ it is common to choose the supremum norm defined by
\[
  \norm{f}_X = \sup_{x \in A} f(x).
\]

Now we use these definitions to derive necessary, \textit{and only necessary},
conditions for the existence of local minimisers.

\begin{thm}
  [Necessary conditions for local minimisers]
  \label{thm:cv-minimisers}

  Let $X$ be a function space and $\calA \subset X$ be a set of admissible
  functions. Let $\overline{u} \in \calA$ be a local minimiser of the
  functional $\calI$. Then,
  \begin{enumerate}
    \item $\delta \calI(\overline{u})(\varphi) = 0$ for every admissible
      perturbation $\varphi$, and
    \item $\delta^2 \calI(\overline{u})(\varphi) > 0$ for every admissible
      perturbation $\varphi$.
  \end{enumerate}
\end{thm}

\begin{proof}
  Define $g: (-\varepsilon_0, \varepsilon_0) \to \R$ be defined as
  $g(\varepsilon) = \calI(\overline{u} + \varepsilon \varphi)$. Then $0$ is a
  local minimiser for $g$. Hence $g'(0) = 0$ and $g''(0) = 0$.
\end{proof}

If you are curious, the reason for \autoref{thm:cv-minimisers} not providing
necessary conditions is that the dimension of the functions space $X$ may be
infinite and then things fail. Keep in mind that this theorem can only be used for two things
\begin{itemize}
  \item Proving that a given $\overline{u}$ is not a local minimiser if it does
    not fulfill conditions 1 and 2 in \autoref{thm:cv-minimisers} (using the
    negated reciprocal), and
  \item Finding candidates for functions $u \in \calA$ to be local minimisers.
    This is not very useful.
\end{itemize}
Also keep in mind that if one wants to prove that a function which satisfies
the hypotheses in \autoref{thm:cv-minimisers} is not a local minimiser one
cannot use the theorem but rather has to come up with something else.

\subsection{The Euler-Lagrange theorem}

Now we go back to the kinds of problems we stated at the beginning of the
chapter: variational problems of integral form. Let us revisit the setting.

Consider a variational problem of the form
\begin{equation}
  \label{eq:cv-integral-2}
  \text{minimise } \calI(u) = \int_a^b f(x, u(x), u'(x))\ dx,
\end{equation}
where $u \in \calA = \{u \in X \mid u(a) = u_a,\ u(b) = u_b\}$ and $X = C^1([a,
b], \R^m)$ and thus $f : [a, b] \times \R^m \times \R^m \to \R$.  Additionally,
for notation purposes, let us assume $f$ is defined by $(x, z, p) \mapsto f(x,
z, p)$.

\paragraph{A comment about the admissible perturbations and first variations.}
In this case where we have set boundary conditions for $u \in \calA$ the set of
admissible perturbations is
\[
  \{ \varphi \in X \mid \varphi(a) = \varphi(b) = 0\},
\]
since that is the only way we can get $u + \varepsilon \varphi$ to be in
$\calA$. On the other hand, if we hand not imposed boundary conditions on $u
\in \calA$, the set of admissible perturbations would just be $X$, since sum
and product with a scalar are closed under function (read vector) spaces.

Let's look at how the first variation of a functional of the form of
\eqref{eq:cv-integral-2} looks like.

\begin{lem}
  Let $X, \calA$ and $\calI$ be defined as in \eqref{eq:cv-integral-2}. Then
  \begin{equation}
    \label{eq:fv-integral}
    \delta \calI (u) (\varphi)
    = \int_a^b \partial_p f(x, u, u') \varphi'(x) + \partial_z f(x, u, u') \varphi\ dx.
  \end{equation}
\end{lem}

\begin{proof}
  TODO
\end{proof}




We are almost ready to introduce the Euler-Lagrange theorem, but first we will
state an important result in the fields of calculus of variations which will
aid us in the proof of the Euler-Lagrange theorem and that will sometimes come
handy when checking for the conditions in \autoref{thm:cv-minimisers}.

\begin{thm}
  [Fundamental theorem of calculus of variations]
  \label{thm:cv-fundamental}

  Let $(a, b) \subset \R$ and $v \in C^0((a, b); \R^m)$ such that
  \[
    \int_a^b v(x) \cdot \varphi(x) \ dx = 0 \text{ for all } \varphi \in C_c^\infty ((a, b); \R^m).
  \]
  Then $v = 0$ in $(a, b)$.
\end{thm}

As a reminder, in the above theorem $C_c^\infty((a, b); \R^m)$ denotes the set
of all infinitely many times continuously differentiable functions with compact
support. Also, in this context we are using the closed version of support since
$\R$ is a topological space, i.e. we define the support of a function $f : \R
\to \R^m$ as the set
\[
  \supp f = \overline{ \{ x \in \R \mid f(x) \neq 0 \} }.
\]
Finally, I would like to clarify that the product inside the integral above $v
\cdot \varphi$ refers to the standard dot product. One may to thing of the
function $v$ as being a row vector and $\varphi$ as a column vector.

\begin{proof}
  See\footnote{The reason I'm not citing the proof in
  \cite{eck2017mathematical} is that the notation is suboptimal to say the
  least...} \cite[p. 6]{jost1998calculus}.
\end{proof}

Without further ado...

\begin{thm}
  [Euler-Lagrange theorem]

  Let $X$ be a functions space, $\calA \subset X$ be a set of admissible
  functions, $\calI: X \to \R$ a functional over $x$ and $f \in C^2([a, b]
  \times \R^m, \times \R^m)$ a real-valued function defined by $(x, z, p)
  \mapsto f(x, z, p)$. If $u$ is a critical point of $\calI$ with $u \in
  C^2([a, b]; \R^m)$ then
  \begin{align}
    \label{eq:euler-lagrange}
    \ddx \partial_p f(x, u(x), u'(x)) = \partial_z f(x, u(x), u'(x)) \text{ for all } x \in [a, b].
  \end{align}
\end{thm}

In other words, the Euler-Lagrange theorem is giving alternative necessary
conditions for $u$ to be a critical point of $\calI$.

\begin{proof}
  We use the necessary condition 1 on \autoref{thm:cv-minimisers} to state $\delta \calI(u)(\varphi) = 0$. Now, using \eqref{eq:fv-integral} we write
  \[
    0 = \delta \calI(u) (\varphi)
    = \int_a^b \partial_p f(x, u, u') \varphi'(x) + \partial_z f(x, u, u') \varphi\ dx.
  \]
  The presence of $\varphi'$ is preventing us from applying
  \autoref{thm:cv-fundamental} so lets try to get rid of it integrating by
  parts
  \[
    0 = \dots
      = \int_a^b \left[ - \ddx \partial_p f(x, u, u') + \partial_z f(x, u, u') \right] \varphi \ dx
        + \left. \partial_p f(x, u, u') \right|_a^b.
  \]
  Now recall that the boundary conditions for $\varphi$ are $\varphi(a) =
  \varphi(b) = 0$ and hence the term $\left. \partial_p f\right|_a^b = 0$.
    Using \autoref{thm:cv-fundamental} we conclude
    \[
      -\ddx \partial_p f(x, u, u')x + \partial_z f(x, u, u') = 0
      \text{ for all } x \in (a, b)
    \]
    and the claim follows.
\end{proof}

An alternative to this proof is to use a more general version of the theorem of
calculus of variations which states that if $\int_a^b fh + gh'\ dx = 0$ for
every suitable $h$ then $g$ is differentiable and $g' = f$ in $(a, b)$. The
version in \autoref{thm:cv-fundamental} is the special case $g = 0$. For more
see \cite{wikicvfundamental}. 

\begin{remark}
  The previous theorem can be strengthened by only requiring that $u \in C^1$
  instead of $C^2$. In this case the du Bois-Reymond
  lemma\footnote{Coincidentally, Paul David Gustav du Bois-Reymond (2 December
  1831 â€“ 7 April 1889) was just one person.} is used for the proof in place of
  \autoref{thm:cv-fundamental}. See \cite{dubois}.
\end{remark}








\section{Exercises}

\begin{ex}
  [Dido's problem]

  Let $L > 0$ be a given length. We consider the maximisation problem
  \[
    \text{maximise } \int_0^L u(s)\sqrt{1 - u'(s)^2}ds, \text{ for } u \in \calA,
  \]
  where $\calA = \{C^1((0, L)) \cap C^0([0, L]) : u(0) = 0, u(L) = 0,
  \abs{u'(s)} \text{ for } s \in (0, L)\}$, which emerges from modeling Dido's
  problem.

  \begin{enumerate}
    \item Determine the corresponding Euler-Lagrange equation and find a
      non-negative solution $\overline{u} \in \calA \cap C^2((0, L))$.
    \item Sketch the curve $\{(\varphi(s), \overline{u}(s)) : s \in [0, L]\}$
      with $\varphi(s) = \int_0^s \sqrt{1 - \overline{u}'(\tau)}d\tau$ for $s
      \in [0, L]$.
    \item Interpret b) in the context of Dido's problem.
  \end{enumerate}

  \textit{Hint for a):} \footnote{In the original statement for this problem,
  the hint was only a simple implication, but by proving it one realised that
it was a double implication.}Prove and use the following statement: Let $a, b
\in \R$ with $a < b,\ u \in C^2((a, b))$ and $f \in C^2(\R \times \R)$ such
that $\partial_p f(u, u') \in C^1(a, b)$. Then
  \[
    \ddx \partial_p f(u, u') = \partial_z f(u, u')\text{ in } (a, b)
  \]
  if, and only if, there exists $c \in \R$ such that
  \[
    f(u, u') - u' \partial_p f(u, u') = c\text{ in } (a, b).
  \]
\end{ex}

\begin{proof}
  [Proof of the hint]
  Not very precise but something along the lines of
  \begin{align*}
         & \ddx \partial_p f(u, u') = \partial_z f(u, u') \\
    \iff & \partial_z f(u, u') - \ddx \partial_p f(u, u') = 0 \\
    \iff & \int \partial_z f(u, u') - \int \ddx \partial_p f(u, u') = \int 0 \\
    \iff & f(u, u') - u' \partial_p f(u, u') = c.
  \end{align*}

  Or, with derivatives,
  \begin{align*}
    & f(u, u') - u'\partial_p f(u, u') = c \\
    \iff & \ddx f(u, u') - \ddx u' \partial_p f(u, u') = \ddx c \\
    \iff & \partial_z f(u, u') u' + \partial_p f(u, u')u'' - u'' \partial_z f(u, u') - u' \ddx \partial_p f(u, u') = 0 \\
    \iff &u' \left( \partial_z f(u, u') - \ddx \partial_p f(u, u')\right) = 0.
  \end{align*}
  In this case, if $u' \neq 0$ we have
  \[
  \partial_z f(u, u') - \ddx \partial_p f(u, u') = 0 \iff
  \ddx \partial_p f(u, u') = \partial_z f(u, u').
  \]
  Otherwise, we go back to the original equation
  \[
    f(u, u') - 0 \partial_p f(u, u') = c \iff  f(u, u') = c
  \]
  which means that $f$ is constant in $u$ and therefore in $x$ so it is trivial to see that
  \[
    \partial_p f(u, u') = 0 = \partial_z f(u, u'),
  \]
  which gives the first equation.
\end{proof}


\begin{proof}
  For the purposes of determining the Euler-Lagrange equation we have $x = s,\
  z = u(x),\ p = u'(s)$ and $f(x, z, p) = z\sqrt{1 - p^2}$. The involved
  partial derivatives are
  \[
    \partial_z f(x, z, p) = \sqrt{1 - p^2} \text{ and }
    \partial_p f(x, z, p) = - \frac{zp}{\sqrt{1 - p^2}},
  \]
  and thus the Euler-Lagrange equation becomes
  \[
    \ddx \frac{- u u'}{\sqrt{1 - u'^2}} = \sqrt{1 - u'^2}.
  \]
  This looks complicated so we apply the hint:
  \begin{align*}
    & u\sqrt{1 - u'^2} + u' \frac{uu'}{\sqrt{1 - u'^2}} = c \\
    \iff & u \left( \sqrt{1 - u'^2} + \frac{u'^2}{\sqrt{1 - u'^2}}\right) = c \\
    \iff & u \left( \frac{1 - u'^2 + u'^2}{\sqrt{1 - u'^2}}\right) = c \\
    \iff & u = c\sqrt{1 - u'^2} \\
    \iff & u' = \sqrt{1 - \left( \frac{u}{c} \right)^2}.
  \end{align*}
  That last equation desperately screams for separation of variables and a
  trigonometric change of variable:
  \begin{align*}
    & \frac{du}{ds} = \sqrt{1 - \left( \frac{u}{c} \right)^2} \\
    \iff &\frac{du}{\sqrt{1 - (u/c)^2}} = ds \\
    \iff & \int \frac{1}{\sqrt{1 - (u/c)^2}} du = \int ds.
  \end{align*}
  Change $u / c = \sin y \implies u = c\sin y \implies du = c\cos y dy$ to get
  \begin{align*}
    \int \frac{1}{\sqrt{1 - (u/c)^2}} du
  & = \int \frac{1}{1 - \sin^2 y} c \cos y dy \\
  &= c \int \frac{\cos y}{\cos y} dy = cy \\
  &= c\arcsin\frac{u}{c}
  \end{align*}
  Plugging it back into the hint,
  \[
    s + k = c\arcsin\frac{u}{c} \implies u(s) = c\sin \frac{k+s}{c},
  \]
  which we rewrite picking different constants,
  \[
    u(s) = k_1 \sin( k_2 s + k_3),
  \]
  where the constants $k_1, k_2, k_3$ are obtained by enforcing $u \in \calA$.
  More specifically, we require
  \[
    u(0) = 0 \implies k_3 = 0\text{ and } u(L) = 0 \implies k_2 L = n\pi.
  \]
  Moreover, we require $u$ to be non-negative, therefore $n = 1$ and thus $k_2
  = \frac{\pi}{L}$ (otherwise the sine would go negative). Finally, we want
  \[
    \abs{u'(s)} < 1 \implies \abs{k_1\cos(k_2s + k_3) k_2} < 1
    \implies k_1 k_2 < 1 \implies k_1 < \frac{L}{\pi},
  \]
  since we require that $k_1$ is positive so that $u(s)$ also is. This final
  parameter is fixed by maximising $\calI(u)$:

  TODO
\end{proof}


\begin{ex}
  [Geodesics in $\R^2$]

  Let $A$ and $B$ be two points in the plane. What is the shortest connection between $A$ and $B$?

  \begin{enumerate}
    \item Set up the variational problem to model the situation.
    \item Solve the problem and interpret the result.
  \end{enumerate}
\end{ex}

\begin{proof}
  Let $X = C^1([0,1]; \R^2)$ and $\calA = \{ u \in X \mid u(0) = A,\ u(1) =
  B\}$. We define our functional $\calI$ as the length of the parametrised
  curve $u$ as follows
  \[
    \calI(u) = \int_0^1 \norm{u'(t)} dt.
  \]
  Our variational problem is
  \[
    \text{minimise } \calI(u) \text{ for } u \in \calA.
  \]

  To solve it we use the Euler-Langrange method. We have $f(t, u(t), u'(t) =
  \norm{u'(t)}$ and, in the form of the Euler-Langrange equations, we get $f(x,
  z, p) = \norm{p}$. Therefore,
  \[
    \delta_z f = 
    \begin{pmatrix}
      0 & 0
    \end{pmatrix}
    \text{ and } \delta_p f =
    \begin{pmatrix}
      \frac{p_1}{\norm{p}} & \frac{p_2}{\norm{p}}
    \end{pmatrix}.
  \]
  Notice how $p = u' : [0, 1] \to \R^2$ so by $\delta_p f$ we really mean the
  last two numbers in $Df$ (which is a row matrix since $f$ is real valued). We
  arrive at the following Euler-Lagrange equation
  \[
    \ddt \delta_p f = \delta_z \iff \ddt
    \begin{pmatrix}
      \frac{u_1}{\norm{u}} & \frac{u_2}{\norm{u}} 
    \end{pmatrix}
    =
    \begin{pmatrix}
      0 & 0
    \end{pmatrix}.
  \]
  Instead of taking the derivative with respect to $t$, we may simply rewrite
  this as
  \[
  \begin{pmatrix}
    \frac{u_1}{\norm{u}} & \frac{u_2}{\norm{u}}
  \end{pmatrix}
  =
  \begin{pmatrix}
    c_1 & c_2
  \end{pmatrix},
  \]
  where $c_1, c_2 \in \R$ are constants.

  Therefore
  \[
    u(t) = \int u'(t) dt = (c_1 t, c_2 t) + u_0,\ u_0 \in \R^2,
  \]
  which is a parametrisation for a curve. The parameters $c_1, c_2$ and $u_0$
  are determined by enforcing $u \in \calA$:
  \[
    u(0) = u_0 = A,\ u(1) = (c_1, c_2)  + u_0 = B.
  \]
\end{proof}
